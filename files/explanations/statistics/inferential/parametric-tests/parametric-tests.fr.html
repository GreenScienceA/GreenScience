<h2>ğŸ“Š Tests ParamÃ©triques : Le Pilier de l'InfÃ©rence Statistique</h2>

<p>Imaginez-vous en 1904 Ã  Dublin, en Irlande. Un jeune chimiste diplÃ´mÃ© d'Oxford nommÃ© <strong>William Sealy Gosset</strong> fixe un tableau de chiffres â€” la teneur en rÃ©sines molles mesurÃ©e sur une poignÃ©e d'Ã©chantillons de houblon destinÃ©s Ã  la brasserie Guinness. Il doit savoir : ce lot de houblon satisfait-il aux normes de qualitÃ© ? Le problÃ¨me ? Il n'a que 5 mesures, et les mÃ©thodes statistiques de son Ã©poque â€” conÃ§ues pour l'astronomie et les donnÃ©es de recensement avec des centaines d'observations â€” sont dÃ©sespÃ©rÃ©ment peu fiables pour de si petits Ã©chantillons. La lutte de Gosset avec cette question pratique allait le conduire Ã  inventer le test statistique le plus utilisÃ© de toute la science : le <strong>test t de Student</strong>.</p>

<p>Les tests paramÃ©triques sont l'Ã©pine dorsale de la statistique infÃ©rentielle. Ils mÃ©ritent le qualificatif de Â« paramÃ©triques Â» car ils font des <em>hypothÃ¨ses sur les paramÃ¨tres</em> de la distribution de la population â€” typiquement que les donnÃ©es suivent une loi normale de moyenne Î¼ et de variance ÏƒÂ² inconnues (Lehmann &amp; Romano, 2005). Quand ces hypothÃ¨ses sont raisonnablement satisfaites, les tests paramÃ©triques sont les outils les plus <strong>puissants</strong> disponibles (Sheskin, 2011).</p>

<blockquote>ğŸ’¡ <strong>L'IdÃ©e Fondamentale :</strong> Les tests paramÃ©triques utilisent les statistiques de l'Ã©chantillon (moyennes, variances) pour faire des infÃ©rences sur les paramÃ¨tres de la population sous des hypothÃ¨ses distributionnelles explicites. Ils Ã©changent des hypothÃ¨ses contre de la puissance â€” en supposant quelque chose sur la forme de la distribution des donnÃ©es, ils extraient plus d'information de chaque observation que les alternatives non paramÃ©triques.</blockquote>

<hr>

<h2>ğŸº BrÃ¨ve Histoire : De la BiÃ¨re Ã  la Biostatistique</h2>

<h3>Gosset et la Naissance du Test t (1908)</h3>

<p>William Sealy Gosset (1876â€“1937) fut embauchÃ© par la brasserie Guinness en 1899. Son travail exigeait de dÃ©terminer si des lots d'orge, de houblon et de malt satisfaisaient aux spÃ©cifications â€” mais ses expÃ©riences ne produisaient que 3 Ã  5 mesures par lot. La thÃ©orie statistique existante nÃ©cessitait de grands Ã©chantillons (Student, 1908).</p>

<p>En 1906, Guinness accorda Ã  Gosset un congÃ© sabbatique au laboratoire biomÃ©trique de Karl Pearson Ã  l'University College London. Sa dÃ©couverte majeure fut que lorsqu'on estime Ã  la fois la moyenne <em>et</em> l'Ã©cart-type Ã  partir d'un petit Ã©chantillon, le ratio rÃ©sultant ne suit pas la loi normale â€” il suit une nouvelle distribution Ã  queues plus lourdes qui dÃ©pend de la taille de l'Ã©chantillon (Student, 1908). Comme Guinness interdisait Ã  ses employÃ©s de publier sous leur vrai nom, Gosset publia son article fondateur sous le pseudonyme <strong>Â« Student Â»</strong> dans <em>Biometrika</em>.</p>

<p>La communautÃ© mathÃ©matique ignora d'abord l'article de Gosset. Ce n'est qu'aprÃ¨s que <strong>Ronald A. Fisher</strong> fournit une dÃ©monstration rigoureuse de la distribution t en 1925 et popularisa le test t dans son ouvrage influent <em>Statistical Methods for Research Workers</em> que la mÃ©thode devint standard (Fisher, 1925).</p>

<blockquote>ğŸ§  <strong>Fisher sur Gosset :</strong> AprÃ¨s la mort de Gosset en 1937, Fisher Ã©crivit qu'il avait introduit Â« une approche fondamentalement nouvelle du problÃ¨me classique de la thÃ©orie des erreurs. Â» Cette apprÃ©ciation continue plus d'un siÃ¨cle plus tard â€” le test t reste la mÃ©thode statistique la plus frÃ©quemment utilisÃ©e dans toutes les disciplines scientifiques.</blockquote>

<hr>

<h2>ğŸ”§ PrÃ©requis : Les Quatre HypothÃ¨ses des Tests ParamÃ©triques</h2>

<table>
  <thead>
    <tr><th>#</th><th>HypothÃ¨se</th><th>Signification</th><th>Comment VÃ©rifier</th><th>ConsÃ©quence de la Violation</th></tr>
  </thead>
  <tbody>
    <tr><td>1</td><td>ğŸ”¢ <strong>Ã‰chelle de Mesure</strong></td><td>DonnÃ©es d'intervalle ou de ratio</td><td>Examiner la nature de la variable</td><td>Moyennes et variances deviennent sans signification</td></tr>
    <tr><td>2</td><td>ğŸ² <strong>IndÃ©pendance</strong></td><td>Les observations sont indÃ©pendantes les unes des autres</td><td>Revue du plan expÃ©rimental ; test de Durbin-Watson</td><td><strong>SÃ©vÃ¨re</strong> â€” inflation de l'erreur de Type I</td></tr>
    <tr><td>3</td><td>ğŸ“ <strong>NormalitÃ©</strong></td><td>La distribution d'Ã©chantillonnage de la moyenne est approximativement normale</td><td>Test de Shapiro-Wilk, diagramme Q-Q, histogramme</td><td><strong>LÃ©gÃ¨re</strong> pour n â‰¥ 25â€“30 ; le test t est robuste (Glass et al., 1972)</td></tr>
    <tr><td>4</td><td>âš–ï¸ <strong>HomogÃ©nÃ©itÃ© des Variances</strong></td><td>Les groupes comparÃ©s ont des variances Ã©gales (homoscÃ©dasticitÃ©)</td><td>Test de Levene, test de Brown-Forsythe</td><td><strong>ModÃ©rÃ©e Ã  sÃ©vÃ¨re</strong> quand les tailles sont inÃ©gales ; utiliser la correction de Welch</td></tr>
  </tbody>
</table>

<blockquote>âš ï¸ <strong>Robustesse â€” Bonne Nouvelle :</strong> Des dÃ©cennies de recherche par simulation ont montrÃ© que le test t est remarquablement <strong>robuste</strong> aux violations de la normalitÃ©, surtout avec des tailles d'Ã©chantillon modÃ©rÃ©es (n â‰¥ 25â€“30) et des groupes de taille Ã©gale (Glass, Peckham, &amp; Sanders, 1972 ; Boneau, 1960). Le thÃ©orÃ¨me central limite garantit que les moyennes d'Ã©chantillon s'approchent de la normalitÃ© quelle que soit la forme de la population.</blockquote>

<hr>

<h2>ğŸ“ Le Test z : Quand Ïƒ Est Connu</h2>

<p style="text-align:center; font-size:1.1em;"><strong>z = (xÌ„ âˆ’ Î¼â‚€) / (Ïƒ / âˆšn)</strong></p>

<p>Sous Hâ‚€, cette statistique suit la loi normale centrÃ©e rÃ©duite N(0, 1).</p>

<p><strong>Utilisation :</strong> Le test z est principalement utilisÃ© quand le Ïƒ de la population est vraiment connu (rare) et pour les tests de proportions avec grands Ã©chantillons.</p>

<h3>Exemple : Scores Ã  un Examen StandardisÃ©</h3>

<p>Un examen national a Î¼â‚€ = 500 et Ïƒ = 100. Un Ã©chantillon de n = 64 Ã©tudiants d'un nouveau programme obtient xÌ„ = 520. z = (520 âˆ’ 500) / (100/âˆš64) = 20 / 12.5 = <strong>1.60</strong>. La valeur critique bilatÃ©rale Ã  Î± = 0.05 est z* = Â±1.96. Puisque |1.60| &lt; 1.96, on <strong>ne rejette pas Hâ‚€</strong>.</p>

<hr>

<h2>ğŸ“ Test t de Student : Le Cheval de Bataille de la Science</h2>

<p>En pratique, on ne connaÃ®t presque jamais Ïƒ. Quand on le remplace par s, la statistique standardisÃ©e suit la <strong>distribution t de Student</strong> Ã  (n âˆ’ 1) degrÃ©s de libertÃ© â€” plus lourde aux queues que la loi normale. Pour n â‰¥ 30, les deux sont virtuellement indiscernables (Student, 1908 ; Fisher, 1925).</p>

<h3>1. Test t pour un Ã‰chantillon</h3>

<p><strong>But :</strong> Tester si Î¼ est Ã©gal Ã  une valeur spÃ©cifiÃ©e Î¼â‚€.</p>

<p style="text-align:center; font-size:1.1em;"><strong>t = (xÌ„ âˆ’ Î¼â‚€) / (s / âˆšn)</strong> &nbsp; avec dl = n âˆ’ 1</p>

<p><strong>Exemple :</strong> Un pÃ©dologue prÃ©lÃ¨ve n = 20 Ã©chantillons Ã  SÃ©tif. pH moyen : xÌ„ = 7.8, s = 0.6. Le sol diffÃ¨re-t-il du pH neutre (Î¼â‚€ = 7.0) ? t = 0.8 / 0.134 = <strong>5.96</strong>. Avec dl = 19, t* = 2.093. On <strong>rejette Hâ‚€</strong>. d de Cohen = 1.33 (trÃ¨s grand).</p>

<h3>2. Test t pour Deux Ã‰chantillons IndÃ©pendants</h3>

<p><strong>Variances Ã©gales (Student original) :</strong></p>
<p style="text-align:center; font-size:1.1em;"><strong>t = (xÌ„â‚ âˆ’ xÌ„â‚‚) / (s<sub>p</sub> Â· âˆš(1/nâ‚ + 1/nâ‚‚))</strong></p>

<p><strong>Variances inÃ©gales (test t de Welch) :</strong></p>
<p style="text-align:center; font-size:1.1em;"><strong>t = (xÌ„â‚ âˆ’ xÌ„â‚‚) / âˆš(sâ‚Â²/nâ‚ + sâ‚‚Â²/nâ‚‚)</strong></p>

<blockquote>ğŸ¯ <strong>Bonne Pratique Moderne :</strong> Delacre, Lakens et Leys (2017) ont dÃ©montrÃ© de maniÃ¨re convaincante que le test t de Welch devrait Ãªtre utilisÃ© <em>par dÃ©faut</em> au lieu du test t de Student. La version de Welch contrÃ´le bien les erreurs de Type I mÃªme quand les variances sont inÃ©gales, et elle perd trÃ¨s peu de puissance quand les variances sont effectivement Ã©gales.</blockquote>

<p><strong>Exemple :</strong> Un agronome compare le rendement de blÃ© (t/ha) entre deux mÃ©thodes d'irrigation. Groupe 1 (goutte-Ã -goutte) : nâ‚ = 15, xÌ„â‚ = 4.5, sâ‚ = 0.7. Groupe 2 (submersion) : nâ‚‚ = 18, xÌ„â‚‚ = 3.9, sâ‚‚ = 0.9. Test de Welch : t = 0.6 / 0.278 = <strong>2.16</strong>, dl â‰ˆ 31.4. On <strong>rejette Hâ‚€</strong>. d = 0.74 (moyen Ã  grand).</p>

<h3>3. Test t pour Ã‰chantillons AppariÃ©s</h3>

<p><strong>But :</strong> Comparer deux mesures sur les <em>mÃªmes</em> sujets.</p>

<p style="text-align:center; font-size:1.1em;"><strong>t = dÌ„ / (s<sub>d</sub> / âˆšn)</strong> &nbsp; avec dl = n âˆ’ 1</p>

<p><strong>Pourquoi il est plus puissant :</strong> En analysant les diffÃ©rences, le test appariÃ© <em>Ã©limine la variabilitÃ© inter-sujets</em>. Chaque sujet sert de son propre contrÃ´le (Cohen, 1988).</p>

<hr>

<h2>ğŸ“‹ Guide de DÃ©cision Complet du Test t</h2>

<table>
  <thead>
    <tr><th>ScÃ©nario</th><th>Test</th><th>Hâ‚€</th><th>Formule</th><th>dl</th></tr>
  </thead>
  <tbody>
    <tr><td>Un Ã©chantillon vs. valeur connue</td><td>t uniÃ©chantillon</td><td>Î¼ = Î¼â‚€</td><td>(xÌ„ âˆ’ Î¼â‚€) / (s/âˆšn)</td><td>n âˆ’ 1</td></tr>
    <tr><td>Deux groupes indÃ©pendants, var. Ã©gales</td><td>t de Student</td><td>Î¼â‚ = Î¼â‚‚</td><td>(xÌ„â‚ âˆ’ xÌ„â‚‚) / (s<sub>p</sub>âˆš(1/nâ‚+1/nâ‚‚))</td><td>nâ‚ + nâ‚‚ âˆ’ 2</td></tr>
    <tr><td>Deux groupes indÃ©pendants, var. inÃ©gales</td><td>t de Welch âœ…</td><td>Î¼â‚ = Î¼â‚‚</td><td>(xÌ„â‚ âˆ’ xÌ„â‚‚) / âˆš(sâ‚Â²/nâ‚+sâ‚‚Â²/nâ‚‚)</td><td>Welch-Satterthwaite</td></tr>
    <tr><td>MÃªmes sujets, deux conditions</td><td>t appariÃ©</td><td>Î¼<sub>d</sub> = 0</td><td>dÌ„ / (s<sub>d</sub>/âˆšn)</td><td>n âˆ’ 1</td></tr>
    <tr><td>Grand Ã©chantillon, Ïƒ connu</td><td>Test z</td><td>Î¼ = Î¼â‚€</td><td>(xÌ„ âˆ’ Î¼â‚€) / (Ïƒ/âˆšn)</td><td>âˆ (normale)</td></tr>
  </tbody>
</table>

<hr>

<h2>ğŸ“ Taille d'Effet : Au-DelÃ  de la SignificativitÃ©</h2>

<h3>d de Cohen</h3>

<p style="text-align:center; font-size:1.1em;"><strong>d = (xÌ„â‚ âˆ’ xÌ„â‚‚) / s<sub>pooled</sub></strong></p>

<table>
  <thead>
    <tr><th>d de Cohen</th><th>InterprÃ©tation</th><th>Analogie</th></tr>
  </thead>
  <tbody>
    <tr><td>0.2</td><td>Petit effet</td><td>DiffÃ©rence de taille entre filles de 15 et 16 ans</td></tr>
    <tr><td>0.5</td><td>Effet moyen</td><td>DiffÃ©rence de taille entre filles de 14 et 18 ans</td></tr>
    <tr><td>0.8</td><td>Grand effet</td><td>DiffÃ©rence de taille entre filles de 13 et 18 ans</td></tr>
  </tbody>
</table>

<blockquote>ğŸ“ <strong>RÃ¨gle de la Â« Nouvelle Statistique Â» :</strong> Toujours rapporter : (1) la statistique de test et les degrÃ©s de libertÃ©, (2) la valeur p exacte, (3) une taille d'effet avec intervalle de confiance, et (4) les statistiques descriptives. Exemple : Â« L'irrigation goutte-Ã -goutte a produit des rendements significativement plus Ã©levÃ©s, t(31.4) = 2.16, p = .038, d = 0.74, IC 95% [0.04, 1.44]. Â» (Cumming, 2014).</blockquote>

<hr>

<h2>âš–ï¸ Test z pour les Proportions</h2>

<h3>Test z uniÃ©chantillon pour une proportion</h3>
<p style="text-align:center; font-size:1.1em;"><strong>z = (pÌ‚ âˆ’ pâ‚€) / âˆš(pâ‚€(1âˆ’pâ‚€)/n)</strong></p>

<h3>Test z biÃ©chantillon pour comparer deux proportions</h3>
<p style="text-align:center; font-size:1.1em;"><strong>z = (pÌ‚â‚ âˆ’ pÌ‚â‚‚) / âˆš(pÌ„(1âˆ’pÌ„)(1/nâ‚ + 1/nâ‚‚))</strong></p>

<p><strong>Exemple :</strong> RÃ©gion A : 420/500 = 84% vaccinÃ©s. RÃ©gion B : 360/500 = 72%. pÌ„ = 0.78. z = 0.12 / 0.0262 = <strong>4.58</strong>. Hautement significatif.</p>

<hr>

<h2>ğŸ” VÃ©rification des HypothÃ¨ses : Guide Pratique</h2>

<h3>Tester la NormalitÃ©</h3>
<table>
  <thead>
    <tr><th>MÃ©thode</th><th>Fonctionnement</th><th>IdÃ©al Pour</th></tr>
  </thead>
  <tbody>
    <tr><td>ğŸ“Š <strong>Shapiro-Wilk</strong></td><td>Test omnibus le plus puissant</td><td>n &lt; 2000</td></tr>
    <tr><td>ğŸ“ˆ <strong>Diagramme Q-Q</strong></td><td>Visuel : les points doivent suivre la diagonale</td><td>Toute taille</td></tr>
    <tr><td>ğŸ“ <strong>AsymÃ©trie &amp; Kurtosis</strong></td><td>Valeurs proches de 0</td><td>VÃ©rification rapide</td></tr>
  </tbody>
</table>

<h3>Tester l'HomogÃ©nÃ©itÃ© des Variances</h3>
<table>
  <thead>
    <tr><th>Test</th><th>Fonctionnement</th><th>Robustesse</th></tr>
  </thead>
  <tbody>
    <tr><td>âš–ï¸ <strong>Levene</strong></td><td>ANOVA sur les dÃ©viations absolues des moyennes de groupe</td><td>ModÃ©rÃ©e</td></tr>
    <tr><td>âš–ï¸ <strong>Brown-Forsythe</strong></td><td>Comme Levene mais utilise les mÃ©dianes</td><td>Bonne â€” recommandÃ© pour donnÃ©es asymÃ©triques</td></tr>
    <tr><td>âš–ï¸ <strong>Bartlett</strong></td><td>Test du rapport de vraisemblance</td><td>Faible â€” trÃ¨s sensible Ã  la non-normalitÃ©</td></tr>
  </tbody>
</table>

<hr>

<h2>ğŸ”„ Quand les HypothÃ¨ses Ã‰chouent : Alternatives</h2>

<table>
  <thead>
    <tr><th>ProblÃ¨me</th><th>RemÃ¨de</th><th>Test Alternatif</th></tr>
  </thead>
  <tbody>
    <tr><td>Non-normalitÃ© (lÃ©gÃ¨re)</td><td>Test t robuste ; procÃ©der pour n â‰¥ 25â€“30</td><td>â€”</td></tr>
    <tr><td>Non-normalitÃ© (sÃ©vÃ¨re, petit n)</td><td>Transformation, bootstrap</td><td>Mann-Whitney (indÃ©pendant) / Wilcoxon (appariÃ©)</td></tr>
    <tr><td>Variances inÃ©gales</td><td>Test t de Welch (recommandation par dÃ©faut)</td><td>â€”</td></tr>
    <tr><td>Valeurs aberrantes</td><td>Moyennes tronquÃ©es (test de Yuen), mÃ©thodes robustes</td><td>Wilcox (2017)</td></tr>
    <tr><td>Non-indÃ©pendance</td><td>ModÃ¨les mixtes, mesures rÃ©pÃ©tÃ©es</td><td>Pas de solution non paramÃ©trique simple</td></tr>
  </tbody>
</table>

<hr>

<h2>ğŸ†š ParamÃ©trique vs. Non ParamÃ©trique</h2>

<table>
  <thead>
    <tr><th>CritÃ¨re</th><th>Tests ParamÃ©triques</th><th>Tests Non ParamÃ©triques</th></tr>
  </thead>
  <tbody>
    <tr><td><strong>HypothÃ¨ses</strong></td><td>NormalitÃ©, homogÃ©nÃ©itÃ© des variances</td><td>Distribution-free</td></tr>
    <tr><td><strong>Niveau de donnÃ©es</strong></td><td>Intervalle / ratio</td><td>Ordinal ou supÃ©rieur</td></tr>
    <tr><td><strong>Puissance (hypothÃ¨ses satisfaites)</strong></td><td>Plus Ã©levÃ©e</td><td>Plus faible</td></tr>
    <tr><td><strong>Puissance (hypothÃ¨ses violÃ©es)</strong></td><td>Peut Ãªtre infÃ©rieure</td><td>Peut Ãªtre supÃ©rieure</td></tr>
    <tr><td><strong>Correspondances</strong></td><td>t Ã  1 Ã©ch., t indÃ©pendant, t appariÃ©, ANOVA</td><td>Test du signe, Mann-Whitney, Wilcoxon, Kruskal-Wallis</td></tr>
  </tbody>
</table>

<hr>

<h2>ğŸ§© SynthÃ¨se</h2>

<p>Les tests paramÃ©triques â€” test z, test t de Student et test t de Welch â€” sont les outils fondamentaux de la comparaison statistique. NÃ©s du besoin pratique de Gosset d'Ã©valuer les ingrÃ©dients de la biÃ¨re avec de petits Ã©chantillons, ils sont devenus les mÃ©thodes les plus appliquÃ©es en mÃ©decine, psychologie, agriculture et ingÃ©nierie. Leur puissance vient de l'exploitation des hypothÃ¨ses distributionnelles â€” mais avec cette puissance vient la responsabilitÃ© de vÃ©rifier ces hypothÃ¨ses et de rapporter les rÃ©sultats complÃ¨tement.</p>

<hr>

<h2>ğŸ“š RÃ©fÃ©rences</h2>

<p>American Psychological Association. (2010). <em>Publication manual of the APA</em> (6e Ã©d.). Author.</p>
<p>Boneau, C. A. (1960). The effects of violations of assumptions underlying the t test. <em>Psychological Bulletin</em>, <em>57</em>(1), 49â€“64.</p>
<p>Cohen, J. (1988). <em>Statistical power analysis for the behavioral sciences</em> (2e Ã©d.). Lawrence Erlbaum.</p>
<p>Cumming, G. (2014). The new statistics: Why and how. <em>Psychological Science</em>, <em>25</em>(1), 7â€“29.</p>
<p>Delacre, M., Lakens, D., &amp; Leys, C. (2017). Why psychologists should by default use Welch's t-test. <em>International Review of Social Psychology</em>, <em>30</em>(1), 92â€“101.</p>
<p>Fisher, R. A. (1925). <em>Statistical methods for research workers</em>. Oliver and Boyd.</p>
<p>Glass, G. V., Peckham, P. D., &amp; Sanders, J. R. (1972). Consequences of failure to meet assumptions. <em>Review of Educational Research</em>, <em>42</em>(3), 237â€“288.</p>
<p>Lehmann, E. L. (2006). <em>Nonparametrics: Statistical methods based on ranks</em>. Springer.</p>
<p>Lehmann, E. L., &amp; Romano, J. P. (2005). <em>Testing statistical hypotheses</em> (3e Ã©d.). Springer.</p>
<p>Levene, H. (1960). Robust tests for equality of variances. In I. Olkin (Ã‰d.), <em>Contributions to probability and statistics</em> (pp. 278â€“292). Stanford University Press.</p>
<p>Sheskin, D. J. (2011). <em>Handbook of parametric and nonparametric statistical procedures</em> (5e Ã©d.). Chapman &amp; Hall/CRC.</p>
<p>Student. (1908). The probable error of a mean. <em>Biometrika</em>, <em>6</em>(1), 1â€“25.</p>
<p>Welch, B. L. (1947). The generalization of "Student's" problem. <em>Biometrika</em>, <em>34</em>(1â€“2), 28â€“35.</p>
<p>Wilcox, R. R. (2017). <em>Introduction to robust estimation and hypothesis testing</em> (4e Ã©d.). Academic Press.</p>
