<h2>ğŸ“‹ Tests Non ParamÃ©triques : Quand vos donnÃ©es refusent de suivre les rÃ¨gles</h2>

<p>Imaginez que vous Ãªtes un chercheur agricole Ã  Batna, en AlgÃ©rie, enquÃªtant auprÃ¨s de 15 petits agriculteurs sur leur satisfaction vis-Ã -vis d'un nouveau systÃ¨me d'irrigation goutte Ã  goutte. Chaque agriculteur note sa satisfaction sur une Ã©chelle de 1 (trÃ¨s insatisfait) Ã  5 (trÃ¨s satisfait). Vous voulez savoir : la satisfaction diffÃ¨re-t-elle entre les agriculteurs ayant reÃ§u une formation et ceux qui n'en ont pas reÃ§u ? Vous attrapez instinctivement un test t â€” puis vous hÃ©sitez. Vos donnÃ©es sont des rangs ordinaux, pas des mesures continues. Avec seulement 7 agriculteurs dans un groupe et 8 dans l'autre, vous ne pouvez pas prÃ©sumer de la normalitÃ©. Les variances semblent trÃ¨s diffÃ©rentes. Toutes les hypothÃ¨ses paramÃ©triques sont violÃ©es. Que faites-vous ?</p>

<p>Vous utilisez un <strong>test non paramÃ©trique</strong> â€” une classe de mÃ©thodes statistiques qui font des hypothÃ¨ses minimales sur la distribution de la population sous-jacente. Ces tests n'exigent pas que vos donnÃ©es soient normalement distribuÃ©es, ne demandent pas l'Ã©galitÃ© des variances et peuvent traiter des donnÃ©es ordinales, classÃ©es ou fortement asymÃ©triques avec rigueur et Ã©lÃ©gance. Ce ne sont ni un compromis ni un Â« Plan B Â» ; c'est une philosophie statistique distincte et puissante avec des fondements mathÃ©matiques profonds, une efficacitÃ© prouvÃ©e et une histoire riche remontant aux annÃ©es 1900.</p>

<blockquote>ğŸ’¡ <strong>L'idÃ©e clÃ© :</strong> Les tests paramÃ©triques sont comme des costumes sur mesure â€” ils s'ajustent parfaitement quand le corps (les donnÃ©es) correspond au patron (les hypothÃ¨ses). Les tests non paramÃ©triques sont comme des vÃªtements Ã©lastiques â€” ils s'adaptent Ã  presque toute morphologie. Vous sacrifiez un peu de prÃ©cision quand le costume aurait parfaitement convenu, mais vous gagnez la capacitÃ© de travailler de maniÃ¨re fiable avec des donnÃ©es de presque n'importe quelle forme distributionnelle.</blockquote>

<hr>

<h2>ğŸ“œ Origines historiques : Du Ï‡Â² de Pearson Ã  la rÃ©volution des rangs</h2>

<p>L'histoire des statistiques non paramÃ©triques est celle d'esprits brillants qui ont remis en question le monopole de la distribution normale sur l'infÃ©rence statistique.</p>

<p>Le plus ancien test non paramÃ©trique prÃ©cÃ¨de le terme lui-mÃªme. En 1900, <strong>Karl Pearson</strong> publia un article fondateur dans le <em>Philosophical Magazine</em> introduisant le <strong>test du chi-deux (Ï‡Â²) d'adÃ©quation</strong> â€” une mÃ©thode qui Ã©value si des frÃ©quences observÃ©es correspondent aux frÃ©quences attendues sans supposer de distribution continue particuliÃ¨re (Pearson, 1900). Le statisticien Bradley Efron a qualifiÃ© cet article de Â« dÃ©but prometteur pour un siÃ¨cle merveilleux dans le domaine des statistiques Â» (Rao, 2002).</p>

<p>En 1904, le psychologue britannique <strong>Charles Spearman</strong> proposa le <strong>coefficient de corrÃ©lation des rangs</strong> (Ï, ou Â« rho de Spearman Â»), qui mesure l'association entre deux variables en utilisant leurs rangs plutÃ´t que leurs valeurs brutes (Spearman, 1904). C'Ã©tait une percÃ©e conceptuelle : en convertissant les observations en rangs, Spearman libÃ©ra l'analyse de corrÃ©lation des hypothÃ¨ses de linÃ©aritÃ© et de normalitÃ©.</p>

<p>La rÃ©volution non paramÃ©trique moderne s'enflamma vÃ©ritablement en 1945, quand <strong>Frank Wilcoxon</strong>, chimiste physicien Ã  l'American Cyanamid Company, publia un article remarquablement concis de quatre pages dans le <em>Biometrics Bulletin</em>. Dans ces quatre pages, il proposa Ã  la fois le <strong>test des rangs signÃ©s</strong> (pour Ã©chantillons appariÃ©s) et le <strong>test de la somme des rangs</strong> (pour Ã©chantillons indÃ©pendants) â€” deux tests qui deviendraient les pierres angulaires des statistiques non paramÃ©triques (Wilcoxon, 1945). La motivation de Wilcoxon Ã©tait pratique : il Ã©tait fatiguÃ© de calculer d'interminables tests t pour ses mesures de laboratoire et voulait des alternatives plus simples basÃ©es sur les rangs (Noether, 1992).</p>

<p>En 1947, <strong>Henry Mann</strong> et son Ã©tudiant <strong>Donald Ransom Whitney</strong> Ã©tendirent le test de la somme des rangs de Wilcoxon aux Ã©chantillons de tailles inÃ©gales et fournirent une analyse thÃ©orique complÃ¨te, crÃ©ant ce qu'on appelle maintenant le <strong>test U de Mann-Whitney</strong> (Mann &amp; Whitney, 1947). Puis en 1952, <strong>William Kruskal</strong> et <strong>W. Allen Wallis</strong> gÃ©nÃ©ralisÃ¨rent le test de rangs Ã  deux Ã©chantillons aux groupes multiples, produisant le <strong>test H de Kruskal-Wallis</strong> â€” l'alternative non paramÃ©trique Ã  l'ANOVA Ã  un facteur (Kruskal &amp; Wallis, 1952). ParallÃ¨lement, l'Ã©conomiste laurÃ©at du prix Nobel <strong>Milton Friedman</strong> avait dÃ©jÃ  proposÃ© un test basÃ© sur les rangs pour les plans Ã  mesures rÃ©pÃ©tÃ©es en 1937, maintenant appelÃ© le <strong>test de Friedman</strong> (Friedman, 1937).</p>

<blockquote>ğŸ§  <strong>La surprise de l'efficacitÃ© :</strong> De nombreux chercheurs supposent que les tests non paramÃ©triques sont intrinsÃ¨quement plus faibles que les tests paramÃ©triques. Mais Hodges et Lehmann (1956) ont prouvÃ© un rÃ©sultat surprenant : l'efficacitÃ© relative asymptotique (ERA) du test de Wilcoxon par rapport au test t <em>ne descend jamais en dessous de 0,864</em>, mÃªme quand les donnÃ©es sont parfaitement normales â€” ce qui signifie que le test de Wilcoxon nÃ©cessite au maximum 16 % d'observations supplÃ©mentaires pour atteindre la mÃªme puissance. Pour les distributions non normales, le test de Wilcoxon peut Ãªtre <em>infiniment</em> plus efficace que le test t.</blockquote>

<hr>

<h2>ğŸ”¬ Le principe fondamental : Les rangs plutÃ´t que les valeurs brutes</h2>

<p>L'intuition fondamentale derriÃ¨re la plupart des tests non paramÃ©triques est d'une simplicitÃ© trompeuse : <strong>remplacer chaque observation par son rang</strong> dans l'Ã©chantillon combinÃ©, puis effectuer les calculs sur ces rangs plutÃ´t que sur les donnÃ©es originales.</p>

<table>
  <thead>
    <tr><th>PropriÃ©tÃ©</th><th>Valeurs brutes</th><th>Rangs</th></tr>
  </thead>
  <tbody>
    <tr><td>SensibilitÃ© aux valeurs extrÃªmes</td><td>Les valeurs extrÃªmes dominent les calculs</td><td>La plus grande valeur obtient le rang <em>n</em>, quelle que soit son amplitude</td></tr>
    <tr><td>HypothÃ¨se de distribution</td><td>Exige une distribution spÃ©cifique (normalement gaussienne)</td><td>Sans distribution sous Hâ‚€</td></tr>
    <tr><td>Ã‰chelle de mesure</td><td>Exige une Ã©chelle d'intervalle/ratio</td><td>Fonctionne avec des donnÃ©es ordinales</td></tr>
    <tr><td>Robustesse</td><td>VulnÃ©rable Ã  la non-normalitÃ©, l'hÃ©tÃ©roscÃ©dasticitÃ©</td><td>TrÃ¨s robuste aux violations distributionnelles</td></tr>
  </tbody>
</table>

<hr>

<h2>ğŸ“Š La famille des tests non paramÃ©triques : Guide complet</h2>

<table>
  <thead>
    <tr><th>Question de recherche</th><th>Test paramÃ©trique</th><th>Alternative non paramÃ©trique</th><th>DonnÃ©es requises</th></tr>
  </thead>
  <tbody>
    <tr><td>Un Ã©chantillon vs. mÃ©diane hypothÃ©tique</td><td>Test t pour un Ã©chantillon</td><td>ğŸ”¹ <strong>Test des rangs signÃ©s de Wilcoxon</strong></td><td>Ordinal+, distribution symÃ©trique des diffÃ©rences</td></tr>
    <tr><td>Deux groupes indÃ©pendants</td><td>Test t indÃ©pendant</td><td>ğŸ”¹ <strong>Test U de Mann-Whitney</strong></td><td>Ordinal+, observations indÃ©pendantes</td></tr>
    <tr><td>Deux groupes appariÃ©s</td><td>Test t appariÃ©</td><td>ğŸ”¹ <strong>Test des rangs signÃ©s de Wilcoxon</strong></td><td>Ordinal+, observations appariÃ©es</td></tr>
    <tr><td>Trois groupes indÃ©pendants ou plus</td><td>ANOVA Ã  un facteur</td><td>ğŸ”¹ <strong>Test H de Kruskal-Wallis</strong></td><td>Ordinal+, observations indÃ©pendantes</td></tr>
    <tr><td>Trois mesures rÃ©pÃ©tÃ©es ou plus</td><td>ANOVA Ã  mesures rÃ©pÃ©tÃ©es</td><td>ğŸ”¹ <strong>Test de Friedman</strong></td><td>Ordinal+, plan en blocs/rÃ©pÃ©tÃ©</td></tr>
    <tr><td>Association entre deux variables</td><td>CorrÃ©lation de Pearson (r)</td><td>ğŸ”¹ <strong>CorrÃ©lation de Spearman (Ï)</strong></td><td>Ordinal+, relation monotone</td></tr>
    <tr><td>Distributions de frÃ©quences catÃ©gorielles</td><td>â€”</td><td>ğŸ”¹ <strong>Test du chi-deux (Ï‡Â²)</strong></td><td>Nominal/catÃ©goriel, effectifs attendus â‰¥ 5</td></tr>
    <tr><td>Tableau 2Ã—2, petits Ã©chantillons</td><td>â€”</td><td>ğŸ”¹ <strong>Test exact de Fisher</strong></td><td>Nominal/catÃ©goriel, toute taille d'Ã©chantillon</td></tr>
  </tbody>
</table>

<hr>

<h2>ğŸ”¹ Test 1 : Le test U de Mann-Whitney (Deux Ã©chantillons indÃ©pendants)</h2>

<h3>Quand l'utiliser</h3>
<p>Utilisez le test U de Mann-Whitney pour comparer deux groupes indÃ©pendants lorsque vous ne pouvez pas supposer la normalitÃ© â€” par exemple, comparer la teneur en matiÃ¨re organique du sol entre exploitations conventionnelles et biologiques lorsque les donnÃ©es sont asymÃ©triques.</p>

<h3>La procÃ©dure</h3>
<p>Supposons que nous mesurions la concentration en cadmium (mg/kg) dans le sol de deux rÃ©gions :</p>
<p><strong>RÃ©gion A (nâ‚ = 5) :</strong> 0,8 ; 1,2 ; 2,5 ; 3,1 ; 4,7<br>
<strong>RÃ©gion B (nâ‚‚ = 5) :</strong> 1,9 ; 3,8 ; 5,2 ; 6,1 ; 8,3</p>

<p><strong>Ã‰tape 1 :</strong> Combiner toutes les observations et attribuer les rangs de 1 Ã  N = 10.<br>
<strong>Ã‰tape 2 :</strong> Calculer les sommes de rangs : Râ‚ = 19, Râ‚‚ = 36.<br>
<strong>Ã‰tape 3 :</strong> Calculer Uâ‚ = nâ‚nâ‚‚ + nâ‚(nâ‚+1)/2 âˆ’ Râ‚ = 25 + 15 âˆ’ 19 = <strong>21</strong> ; Uâ‚‚ = 25 âˆ’ 21 = <strong>4</strong>.<br>
<strong>Ã‰tape 4 :</strong> U = min(Uâ‚, Uâ‚‚) = 4. Comparer Ã  la valeur critique.</p>

<h3>La formule (gÃ©nÃ©rale)</h3>
<p style="text-align:center; font-size:1.1em;"><strong>U = nâ‚nâ‚‚ + nâ‚(nâ‚+1)/2 âˆ’ Râ‚</strong></p>

<p>Pour les grands Ã©chantillons (nâ‚ et nâ‚‚ > 20), U suit approximativement une loi normale avec Î¼<sub>U</sub> = nâ‚nâ‚‚/2 et Ïƒ<sub>U</sub> = âˆš(nâ‚nâ‚‚(nâ‚+nâ‚‚+1)/12) (Mann &amp; Whitney, 1947).</p>

<hr>

<h2>ğŸ”¹ Test 2 : Le test des rangs signÃ©s de Wilcoxon (Ã‰chantillons appariÃ©s)</h2>

<h3>Quand l'utiliser</h3>
<p>Utilisez-le pour comparer deux mesures apparentÃ©es â€” avant/aprÃ¨s traitement, paires appariÃ©es â€” quand vous ne pouvez pas supposer que les diffÃ©rences sont normalement distribuÃ©es.</p>

<h3>La procÃ©dure</h3>
<p>Nous mesurons le pH du sol sur 8 sites avant et aprÃ¨s application d'un amendement calcaire :</p>

<p><strong>Ã‰tape 1 :</strong> Calculer les diffÃ©rences d, Ã©liminer les zÃ©ros.<br>
<strong>Ã‰tape 2 :</strong> Classer les |d| par ordre croissant et attribuer les rangs.<br>
<strong>Ã‰tape 3 :</strong> Attacher le signe de la diffÃ©rence originale Ã  chaque rang.<br>
<strong>Ã‰tape 4 :</strong> Calculer Wâº (somme des rangs positifs) et Wâ» (somme des rangs nÃ©gatifs). T = min(Wâº, Wâ»).</p>

<p>Le test suppose que la distribution des diffÃ©rences est <em>symÃ©trique</em> autour de la mÃ©diane. Il ne requiert <em>pas</em> la normalitÃ©, mais l'hypothÃ¨se de symÃ©trie est importante (Wilcoxon, 1945).</p>

<hr>

<h2>ğŸ”¹ Test 3 : Le test H de Kruskal-Wallis (Trois groupes indÃ©pendants ou plus)</h2>

<h3>La formule</h3>
<p style="text-align:center; font-size:1.1em;"><strong>H = [12 / N(N+1)] Ã— Î£ (Ráµ¢Â²/náµ¢) âˆ’ 3(N+1)</strong></p>

<p>oÃ¹ N est la taille totale de l'Ã©chantillon, náµ¢ la taille du groupe i, et Ráµ¢ la somme des rangs du groupe i (Kruskal &amp; Wallis, 1952).</p>

<p>Sous Hâ‚€, H suit approximativement une distribution Ï‡Â²(kâˆ’1). Comme l'ANOVA, c'est un test <em>omnibus</em> : il indique qu'au moins un groupe diffÃ¨re, mais pas lequel. Des comparaisons post-hoc (par exemple, test de Dunn avec correction de Bonferroni) sont nÃ©cessaires (Conover, 1999).</p>

<hr>

<h2>ğŸ”¹ Test 4 : Le test de Friedman (Mesures rÃ©pÃ©tÃ©es, trois conditions ou plus)</h2>

<p>Le test de Friedman classe les observations <em>au sein de chaque bloc</em> (sujet), puis teste si les sommes de rangs entre traitements diffÃ¨rent plus que ce que le hasard prÃ©dirait (Friedman, 1937).</p>

<p style="text-align:center; font-size:1.1em;"><strong>Ï‡Â²<sub>F</sub> = [12 / bk(k+1)] Ã— Î£ Râ±¼Â² âˆ’ 3b(k+1)</strong></p>

<p>oÃ¹ b = nombre de blocs, k = nombre de traitements, Râ±¼ = somme des rangs pour le traitement j.</p>

<hr>

<h2>ğŸ”¹ Test 5 : Le test du Chi-deux (Ï‡Â²) de Pearson (DonnÃ©es catÃ©gorielles)</h2>

<h3>Trois visages du Ï‡Â²</h3>
<p><strong>â‘  AdÃ©quation (Goodness of Fit) :</strong> Une distribution observÃ©e correspond-elle Ã  une distribution thÃ©orique ?<br>
<strong>â‘¡ Test d'indÃ©pendance :</strong> Deux variables catÃ©gorielles sont-elles associÃ©es dans un tableau de contingence ?<br>
<strong>â‘¢ Test d'homogÃ©nÃ©itÃ© :</strong> Deux groupes ou plus ont-ils la mÃªme distribution d'une variable catÃ©gorielle ?</p>

<h3>La formule</h3>
<p style="text-align:center; font-size:1.15em;"><strong>Ï‡Â² = Î£ (Oáµ¢ âˆ’ Eáµ¢)Â² / Eáµ¢</strong></p>

<p>oÃ¹ Oáµ¢ = frÃ©quence observÃ©e et Eáµ¢ = frÃ©quence attendue sous Hâ‚€ (Pearson, 1900).</p>

<h3>Conditions de validitÃ©</h3>
<p>Le test du Ï‡Â² exige que les frÃ©quences attendues soient suffisamment grandes â€” la rÃ¨gle classique est Eáµ¢ â‰¥ 5 pour toutes les cellules (Cochran, 1954). Pour les tableaux 2Ã—2 avec de petits effectifs attendus, utilisez le <strong>test exact de Fisher</strong>.</p>

<hr>

<h2>ğŸ”¹ Test 6 : La corrÃ©lation de rang de Spearman (Ï)</h2>

<p style="text-align:center; font-size:1.15em;"><strong>Ï = 1 âˆ’ [6Î£dáµ¢Â²] / [n(nÂ²âˆ’1)]</strong></p>

<p>oÃ¹ dáµ¢ = diffÃ©rence entre les rangs des observations correspondantes (Spearman, 1904). Ï varie de âˆ’1 (relation monotone nÃ©gative parfaite) Ã  +1 (relation monotone positive parfaite). Contrairement Ã  la corrÃ©lation de Pearson, Ï de Spearman dÃ©tecte toute relation monotone, pas seulement linÃ©aire (Lehmann &amp; D'Abrera, 1998).</p>

<hr>

<h2>âš–ï¸ EfficacitÃ© relative asymptotique : Combien de puissance perd-on ?</h2>

<table>
  <thead>
    <tr><th>Test non paramÃ©trique</th><th>ComparÃ© Ã </th><th>ERA (donnÃ©es normales)</th><th>ERA (logistique)</th><th>ERA (double exponentielle)</th></tr>
  </thead>
  <tbody>
    <tr><td>Mann-Whitney / Wilcoxon</td><td>Test t</td><td>3/Ï€ â‰ˆ <strong>0,955</strong></td><td>Ï€Â²/9 â‰ˆ <strong>1,098</strong></td><td><strong>1,500</strong></td></tr>
    <tr><td>Test du signe</td><td>Test t</td><td>2/Ï€ â‰ˆ <strong>0,637</strong></td><td>Ï€Â²/12 â‰ˆ <strong>0,822</strong></td><td><strong>2,000</strong></td></tr>
    <tr><td>Kruskal-Wallis</td><td>Test F (ANOVA)</td><td>3/Ï€ â‰ˆ <strong>0,955</strong></td><td>Ï€Â²/9 â‰ˆ <strong>1,098</strong></td><td><strong>1,500</strong></td></tr>
  </tbody>
</table>

<p><em>Source : Hodges &amp; Lehmann (1956).</em></p>

<p><strong>â‘  Le pire des cas n'est pas grave :</strong> MÃªme quand les donnÃ©es sont parfaitement normales, le test de Wilcoxon conserve 95,5 % de l'efficacitÃ© du test t.<br>
<strong>â‘¡ Pour les donnÃ©es non normales, les tests non paramÃ©triques peuvent Ãªtre <em>plus</em> puissants :</strong> Pour les distributions Ã  queues lourdes, l'avantage peut Ãªtre considÃ©rable.<br>
<strong>â‘¢ La garantie minimale d'ERA est de 0,864 :</strong> Dans le pire des cas absolu, il faut au maximum 16 % de donnÃ©es supplÃ©mentaires.</p>

<hr>

<h2>ğŸ¯ Quand choisir les tests non paramÃ©triques : Cadre de dÃ©cision</h2>

<p><strong>âœ… Utilisez les tests non paramÃ©triques quand :</strong></p>
<p>â‘  Vos donnÃ©es sont <strong>ordinales</strong> (rangs, Ã©chelles de Likert)<br>
â‘¡ L'Ã©chantillon est <strong>petit</strong> (n < 20â€“30) et la normalitÃ© ne peut Ãªtre vÃ©rifiÃ©e<br>
â‘¢ Les tests de normalitÃ© (Shapiro-Wilk) <strong>rejettent la normalitÃ©</strong><br>
â‘£ Les donnÃ©es contiennent des <strong>valeurs aberrantes extrÃªmes</strong><br>
â‘¤ Les donnÃ©es sont <strong>fortement asymÃ©triques</strong><br>
â‘¥ Les <strong>variances sont trÃ¨s inÃ©gales</strong> entre les groupes<br>
â‘¦ Vous travaillez avec des <strong>donnÃ©es catÃ©gorielles</strong> (â†’ test Ï‡Â²)</p>

<blockquote>ğŸ” <strong>IdÃ©e fausse courante :</strong> Â« Les tests non paramÃ©triques n'ont pas d'hypothÃ¨ses. Â» C'est faux. Ces tests supposent toujours l'indÃ©pendance des observations, et plusieurs exigent que la distribution des diffÃ©rences soit symÃ©trique (Wilcoxon) ou que les groupes aient des distributions de forme similaire (Mann-Whitney). Ils font simplement des hypothÃ¨ses <em>moins nombreuses et plus faibles</em>.</blockquote>

<hr>

<h2>âš™ï¸ Tailles d'effet pour les tests non paramÃ©triques</h2>

<table>
  <thead>
    <tr><th>Test</th><th>Mesure de taille d'effet</th><th>InterprÃ©tation</th></tr>
  </thead>
  <tbody>
    <tr><td>Mann-Whitney U</td><td><strong>r = Z/âˆšN</strong></td><td>Petit : 0,1 ; Moyen : 0,3 ; Grand : 0,5</td></tr>
    <tr><td>Kruskal-Wallis</td><td><strong>Î·Â²<sub>H</sub> = (H âˆ’ k + 1)/(N âˆ’ k)</strong></td><td>Analogue Ã  Î·Â² de l'ANOVA</td></tr>
    <tr><td>Chi-deux (Ï‡Â²)</td><td><strong>V de CramÃ©r = âˆš(Ï‡Â²/(NÃ—min(râˆ’1, câˆ’1)))</strong></td><td>0 Ã  1 ; petit : 0,1 ; moyen : 0,3 ; grand : 0,5</td></tr>
    <tr><td>Spearman Ï</td><td><strong>Ï lui-mÃªme</strong></td><td>âˆ’1 Ã  +1 ; petit : 0,1 ; moyen : 0,3 ; grand : 0,5</td></tr>
  </tbody>
</table>

<hr>

<h2>âœ… Bonnes pratiques et directives APA de prÃ©sentation</h2>

<p><strong>1.</strong> Rapportez la statistique exacte, les tailles d'Ã©chantillon et la valeur p (APA, 2020).<br>
<strong>2.</strong> Rapportez les <strong>mÃ©dianes</strong> plutÃ´t que les moyennes comme mesure de tendance centrale.<br>
<strong>3.</strong> Incluez toujours une <strong>taille d'effet</strong> (r, Î·Â²<sub>H</sub>, V de CramÃ©r ou Ï).<br>
<strong>4.</strong> <strong>Justifiez votre choix</strong> du test non paramÃ©trique.<br>
<strong>5.</strong> Utilisez des <strong>tests exacts</strong> pour les petits Ã©chantillons (n < 20).<br>
<strong>6.</strong> Appliquez les <strong>corrections pour ex aequo</strong>.<br>
<strong>7.</strong> Pour les comparaisons post-hoc, utilisez le test de Dunn avec correction de Bonferroni.<br>
<strong>8.</strong> Pour les tests Ï‡Â², rapportez les <strong>frÃ©quences attendues</strong> (Cochran, 1954).</p>

<hr>

<h2>ğŸ§© Sujets avancÃ©s et perspectives modernes</h2>

<h3>L'alternative du bootstrap</h3>
<p>Le calcul moderne a permis les <strong>mÃ©thodes bootstrap</strong> (Efron, 1979) â€” des approches par rÃ©Ã©chantillonnage pouvant remplacer ou complÃ©ter les tests paramÃ©triques et non paramÃ©triques classiques. Les tests classiques basÃ©s sur les rangs restent prÃ©fÃ©rÃ©s pour les plans standards grÃ¢ce Ã  leurs propriÃ©tÃ©s exactes sans distribution (Lehmann &amp; D'Abrera, 1998).</p>

<h3>Limites</h3>
<p>Les tests non paramÃ©triques ont des limites rÃ©elles. Il n'existe pas d'Ã©quivalent basÃ© sur les rangs pour une ANOVA factorielle avec interaction, un modÃ¨le mixte ou une ANCOVA. Pour ces plans, les chercheurs peuvent utiliser des mÃ©thodes paramÃ©triques robustes, des modÃ¨les linÃ©aires gÃ©nÃ©ralisÃ©s ou des approches bootstrap (Hollander et al., 2014).</p>

<hr>

<h2>ğŸ“š RÃ©fÃ©rences</h2>

<p>American Psychological Association. (2020). <em>Publication manual of the American Psychological Association</em> (7e Ã©d.). American Psychological Association.</p>

<p>Cochran, W. G. (1954). Some methods for strengthening the common Ï‡Â² tests. <em>Biometrics</em>, <em>10</em>(4), 417â€“451.</p>

<p>Conover, W. J. (1999). <em>Practical nonparametric statistics</em> (3e Ã©d.). John Wiley &amp; Sons.</p>

<p>Efron, B. (1979). Bootstrap methods: Another look at the jackknife. <em>The Annals of Statistics</em>, <em>7</em>(1), 1â€“26.</p>

<p>Friedman, M. (1937). The use of ranks to avoid the assumption of normality implicit in the analysis of variance. <em>Journal of the American Statistical Association</em>, <em>32</em>(200), 675â€“701.</p>

<p>Hodges, J. L., Jr., &amp; Lehmann, E. L. (1956). The efficiency of some nonparametric competitors of the <em>t</em>-test. <em>Annals of Mathematical Statistics</em>, <em>27</em>(2), 324â€“335.</p>

<p>Hollander, M., Wolfe, D. A., &amp; Chicken, E. (2014). <em>Nonparametric statistical methods</em> (3e Ã©d.). John Wiley &amp; Sons.</p>

<p>Kruskal, W. H., &amp; Wallis, W. A. (1952). Use of ranks in one-criterion variance analysis. <em>Journal of the American Statistical Association</em>, <em>47</em>(260), 583â€“621.</p>

<p>Lehmann, E. L., &amp; D'Abrera, H. J. M. (1998). <em>Nonparametrics: Statistical methods based on ranks</em> (Ã©d. rÃ©v.). Prentice-Hall.</p>

<p>Mann, H. B., &amp; Whitney, D. R. (1947). On a test of whether one of two random variables is stochastically larger than the other. <em>Annals of Mathematical Statistics</em>, <em>18</em>(1), 50â€“60.</p>

<p>Noether, G. E. (1992). Introduction to Wilcoxon (1945) Individual comparisons by ranking methods. In S. Kotz &amp; N. L. Johnson (Eds.), <em>Breakthroughs in statistics</em> (pp. 196â€“202). Springer.</p>

<p>Oja, H. (2010). <em>Multivariate nonparametric methods with R</em>. Springer.</p>

<p>Pearson, K. (1900). On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. <em>Philosophical Magazine, Series 5</em>, <em>50</em>(302), 157â€“175.</p>

<p>Rao, C. R. (2002). Karl Pearson chi-square test: The dawn of statistical inference. In C. Huber-Carol et al. (Eds.), <em>Goodness-of-fit tests and model validity</em> (pp. 9â€“24). BirkhÃ¤user.</p>

<p>Spearman, C. (1904). The proof and measurement of association between two things. <em>The American Journal of Psychology</em>, <em>15</em>(1), 72â€“101.</p>

<p>Wilcoxon, F. (1945). Individual comparisons by ranking methods. <em>Biometrics Bulletin</em>, <em>1</em>(6), 80â€“83.</p>
